{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1720338c2ec40d7a12442e927177a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2a63bed66cc449d8c8c7908b00a000e",
              "IPY_MODEL_9f03a81489354b7abd6ea16654f90423",
              "IPY_MODEL_f437011168c94c55a18cc928c1b42477"
            ],
            "layout": "IPY_MODEL_bc5fa5ed59cc4fd6bda4d701e1ad81e2"
          }
        },
        "a2a63bed66cc449d8c8c7908b00a000e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea1dc49ea144e558a540c2ea9914fd9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5ea96385ceba431ab623ab9f3acb86cf",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "9f03a81489354b7abd6ea16654f90423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11753519b5404f9b890770a4ef2c9a25",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5417cda0e31d4b9d82b409279f206635",
            "value": 4
          }
        },
        "f437011168c94c55a18cc928c1b42477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5ae8c28840441369b53333cf9d0b776",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ea44912770324649a562650c35a73833",
            "value": "â€‡4/4â€‡[00:17&lt;00:00,â€‡â€‡3.77s/it]"
          }
        },
        "bc5fa5ed59cc4fd6bda4d701e1ad81e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea1dc49ea144e558a540c2ea9914fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea96385ceba431ab623ab9f3acb86cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11753519b5404f9b890770a4ef2c9a25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5417cda0e31d4b9d82b409279f206635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5ae8c28840441369b53333cf9d0b776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea44912770324649a562650c35a73833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/dragonkue/BGE-m3-ko"
      ],
      "metadata": {
        "id": "2G3wuhljndZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/kakaocorp/kanana-1.5-8b-base"
      ],
      "metadata": {
        "id": "O0BSpS3bnfCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì „ì²´ ì½”ë“œ ì‘ì„±"
      ],
      "metadata": {
        "id": "3_TDWUa3Tmbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ],
      "metadata": {
        "id": "kOe0vKu9Tq5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# ==============================================================================\n",
        "\n",
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install -q datasets sentence-transformers chromadb transformers torch accelerate bitsandbytes pandas langchain_community langchain-core\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from typing import Dict, Any, List\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "L3TTt4dLTmB_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "rSqtwPlBTyMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. ì „ì—­ ì„¤ì • ë° ëª¨ë¸ ì •ë³´\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Global Configurations ---\n",
        "LLM_MODEL_NAME = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n",
        "EMBEDDING_MODEL_NAME = \"dragonkue/bge-m3-ko\"\n",
        "CHROMA_DB_PATH = \"./chroma_db_korquad_rag\"\n",
        "\n",
        "# --- LLM Quantization Configuration (4-bit) ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# --- Global Variables for Loaded Resources ---\n",
        "tokenizer = None\n",
        "model = None\n",
        "unified_db = None\n",
        "\n",
        "# --- Few-Shot/RAG Prompt Templates ---\n",
        "\n",
        "# Few-Shot ì˜ˆì‹œ í…œí”Œë¦¿\n",
        "example_template = \"\"\"\n",
        "[ì˜ˆì‹œ ì§ˆë¬¸]: {question}\n",
        "[ì˜ˆì‹œ ì¶œì²˜]: {context}\n",
        "[ì˜ˆì‹œ ë‹µë³€]: {answer}\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\", \"context\"], template=example_template)\n",
        "\n",
        "# ìµœì¢… RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "final_rag_template_fewshot = \"\"\"\n",
        "## ì„ë¬´:\n",
        "\n",
        "ì œê³µëœ 'ìµœì¢… ì¶œì²˜' ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ 'ìµœì¢… ì§ˆë¬¸'ì— ê°€ì¥ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤. ì¶œì²˜ì— ë‹µì´ ì—†ìœ¼ë©´ 'ì£¼ì–´ì§„ ì •ë³´ë¡œëŠ” ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µí•˜ì„¸ìš”. ë‹µë³€ì€ ë°˜ë“œì‹œ ì¶œì²˜ì— ëª…ì‹œëœ ìš©ì–´ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n",
        "\n",
        "## í•™ìŠµ ì˜ˆì‹œ:\n",
        "{few_shot_examples}\n",
        "\n",
        "## ìµœì¢… ì¶œì²˜:\n",
        "{context}\n",
        "\n",
        "## ìµœì¢… ì§ˆë¬¸:\n",
        "{question}\n",
        "\n",
        "[ìµœì¢… ë‹µë³€]:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WNeWv_q_TvxP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. ì„ë² ë”© ëª¨ë¸ ë° LLM ë¡œë“œ"
      ],
      "metadata": {
        "id": "ulM88QC9T6e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. ë°ì´í„° ì²˜ë¦¬ ë° ë²¡í„° DB êµ¬ì¶• í•¨ìˆ˜\n",
        "# ==============================================================================\n",
        "\n",
        "def setup_database():\n",
        "    \"\"\"KorQuAD ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ChromaDBë¥¼ êµ¬ì¶• ë˜ëŠ” ë¡œë“œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    global unified_db\n",
        "\n",
        "    if os.path.exists(CHROMA_DB_PATH):\n",
        "        print(f\"âœ… ê¸°ì¡´ DB ë¡œë“œ: {CHROMA_DB_PATH}\")\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "        unified_db = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)\n",
        "        return\n",
        "\n",
        "    print(\"DB êµ¬ì¶• ì‹œì‘: KorQuAD ë°ì´í„° ë¡œë“œ ë° ì„ë² ë”©...\")\n",
        "\n",
        "    data = load_dataset('squad_kor_v1', split='train')\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    full_context_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "            full_context_data.append({\n",
        "                \"question\": row['question'],\n",
        "                \"answer\": row['answers']['text'][0],\n",
        "                \"context\": row['context'],\n",
        "                \"title\": row['title']\n",
        "            })\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    processed_df = pd.DataFrame(full_context_data)\n",
        "    rag_df = processed_df.drop_duplicates(subset=['question', 'context'])\n",
        "\n",
        "    rag_docs = []\n",
        "    for index, row in rag_df.iterrows():\n",
        "        rag_docs.append(\n",
        "            Document(\n",
        "                page_content=row['context'],\n",
        "                metadata={\n",
        "                    \"question\": row['question'],\n",
        "                    \"answer\": row['answer'],\n",
        "                    \"title\": row['title']\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "    unified_db = Chroma.from_documents(\n",
        "        rag_docs,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=CHROMA_DB_PATH\n",
        "    )\n",
        "    unified_db.persist()\n",
        "    print(\"âœ… ì›ë¬¸ Context ê¸°ë°˜ í†µí•© DB êµ¬ì¶• ì™„ë£Œ.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qN86XRqcT7Jm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤(ChromaDB) êµ¬ì¶•"
      ],
      "metadata": {
        "id": "ggDQD14jUFMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. LLM ë¡œë“œ ë° Tokenizer ì„¤ì •\n",
        "# ==============================================================================\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"LLM ë° Tokenizerë¥¼ ë¡œë“œí•˜ê³  ì„¤ì •í•©ë‹ˆë‹¤. (ì¤‘ë³µ ë¡œë“œ ë°©ì§€)\"\"\"\n",
        "    global tokenizer, model\n",
        "\n",
        "    if model is not None:\n",
        "        print(\"âœ… ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¡œë”©ì„ ê±´ë„ˆëœœ.\")\n",
        "        return\n",
        "\n",
        "    print(f\"LLM ë¡œë“œ ì¤‘: {LLM_MODEL_NAME}\")\n",
        "\n",
        "    try:\n",
        "        # 1. Tokenizer ì„¤ì • (ì œê³µëœ ëª¨ë¸ ì„¤ì •ê°’ ë°˜ì˜)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "        tokenizer.bos_token_id = 128000\n",
        "        tokenizer.eos_token_id = 128009\n",
        "        tokenizer.pad_token_id = 128001\n",
        "\n",
        "        # 2. LLM ë¡œë“œ (4-bit ì–‘ìí™”)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            LLM_MODEL_NAME,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        model.eval()\n",
        "        print(\"âœ… LLM ë¡œë“œ ë° ì„¤ì • ì™„ë£Œ.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ LLM ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "        raise RuntimeError(\"LLM ë¡œë“œ ì‹¤íŒ¨. Colab í™˜ê²½ (GPU) ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vQxx-ucpUK_8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. RAG ëª¨ë¸ ì„¤ì • ë° ì‹¤í–‰"
      ],
      "metadata": {
        "id": "h4IIa3-2UP0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. RAG ì¶”ë¡  í•¨ìˆ˜ (Few-Shot RAG)\n",
        "# ==============================================================================\n",
        "\n",
        "def few_shot_rag_invoke(query: str, k_fewshot: int = 1, k_rag: int = 4) -> Dict[str, Any]:\n",
        "    \"\"\"Few-Shot ì˜ˆì‹œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìµœì¢… ì¶œì²˜ì— í¬í•¨í•˜ì—¬ RAG ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
        "\n",
        "    if unified_db is None or model is None:\n",
        "        raise RuntimeError(\"RAG ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    print(f\"\\nRAG ì‹¤í–‰: ì§ˆë¬¸='{query}' (Few-Shot K={k_fewshot}, RAG K={k_rag} ë¬¸ì„œ ê²€ìƒ‰)\")\n",
        "\n",
        "    # 1. Few-Shot & RAG Context í†µí•© ê²€ìƒ‰ ë° ì¤‘ë³µ ì œê±°\n",
        "    search_results = unified_db.similarity_search(query, k=k_fewshot + k_rag + 5)\n",
        "\n",
        "    unique_results = []\n",
        "    seen_content = set()\n",
        "    for doc in search_results:\n",
        "        if doc.page_content not in seen_content:\n",
        "            unique_results.append(doc)\n",
        "            seen_content.add(doc.page_content)\n",
        "\n",
        "    # ê²€ìƒ‰ ê²°ê³¼ ë¶„í• : Few-Shot ì˜ˆì‹œì™€ RAG Context\n",
        "    few_shot_results = unique_results[:k_fewshot]\n",
        "    rag_docs = unique_results[k_fewshot:k_fewshot + k_rag]\n",
        "\n",
        "    # 2. Few-Shot ì˜ˆì‹œ í…ìŠ¤íŠ¸ êµ¬ì„± (ëª¨ë¸ í•™ìŠµìš©)\n",
        "    few_shot_examples_text = \"\".join([\n",
        "        example_prompt.format(\n",
        "            question=doc.metadata['question'],\n",
        "            answer=doc.metadata['answer'],\n",
        "            context=doc.page_content\n",
        "        ) for doc in few_shot_results\n",
        "    ])\n",
        "\n",
        "    # 3. RAG Context êµ¬ì„± (ìµœì¢… ì¶œì²˜: Few-Shot ì»¨í…ìŠ¤íŠ¸ + RAG ì»¨í…ìŠ¤íŠ¸)\n",
        "\n",
        "    # Few-Shot ì˜ˆì‹œ ë¬¸ì„œì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
        "    few_shot_contexts = [doc.page_content for doc in few_shot_results]\n",
        "    # RAG ë¬¸ì„œì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´\n",
        "    rag_contexts = [doc.page_content for doc in rag_docs]\n",
        "\n",
        "    # ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ í•©ì³ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³  ë³‘í•©\n",
        "    all_contexts = few_shot_contexts + rag_contexts\n",
        "    rag_context = \"\\n---\\n\".join(all_contexts)\n",
        "\n",
        "    # 4. ìµœì¢… í”„ë¡¬í”„íŠ¸ Content êµ¬ì„± (Few-Shot í…œí”Œë¦¿ ì‚¬ìš©)\n",
        "    final_prompt_content = final_rag_template_fewshot.format(\n",
        "        few_shot_examples=few_shot_examples_text,\n",
        "        context=rag_context,\n",
        "        question=query\n",
        "    )\n",
        "\n",
        "    # 5. LLM ì…ë ¥ ì¤€ë¹„ ë° ì¶”ë¡ \n",
        "    messages = [{\"role\": \"user\", \"content\": final_prompt_content}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128, do_sample=False, eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # 6. ë‹µë³€ ë””ì½”ë”© ë° í´ë¦¬ë‹\n",
        "    decoded_answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip()\n",
        "    clean_answer = decoded_answer\n",
        "    stop_tokens = [\"assistant\", \"[ìµœì¢… ë‹µë³€]:\", \"user:\"]\n",
        "    for token in stop_tokens:\n",
        "        if token in clean_answer:\n",
        "            clean_answer = clean_answer.split(token)[0].strip()\n",
        "\n",
        "    # 7. ìµœì¢… ê²°ê³¼ í¬ë§·íŒ… ë° ì¶œì²˜ í‘œê¸° ì¶”ê°€\n",
        "    source_citation_list = []\n",
        "\n",
        "    # Few-Shot ì˜ˆì‹œ ë¬¸ì„œ í¬ë§·íŒ…\n",
        "    for i, doc in enumerate(few_shot_results):\n",
        "        # ë¬¸ì„œ ì œëª©: ìœ ì‚¬ ì§ˆë¬¸ (Few-Shot ì˜ˆì‹œ) í˜•ì‹\n",
        "        citation_text = f\"[{i+1}] {doc.metadata.get('title', 'ì •ë³´ ì—†ìŒ')}: {doc.metadata.get('question', 'ì§ˆë¬¸ ì—†ìŒ')}\"\n",
        "        source_citation_list.append(citation_text)\n",
        "\n",
        "    # RAG Context ë¬¸ì„œ í¬ë§·íŒ…\n",
        "    rag_doc_start_index = len(few_shot_results)\n",
        "    for i, doc in enumerate(rag_docs):\n",
        "        # ë¬¸ì„œ ì œëª©: ì§ˆë¬¸ í˜•ì‹\n",
        "        citation_text = f\"[{i + rag_doc_start_index + 1}] {doc.metadata.get('title', 'ì •ë³´ ì—†ìŒ')}: {doc.metadata.get('question', 'ì§ˆë¬¸ ì—†ìŒ')}\"\n",
        "        source_citation_list.append(citation_text)\n",
        "\n",
        "    # LLM ë‹µë³€ê³¼ ì¶œì²˜ ëª©ë¡ì„ ê²°í•©\n",
        "    final_answer_with_citation = clean_answer\n",
        "    if source_citation_list:\n",
        "        final_answer_with_citation += \"\\n\\n**-- ì°¸ì¡°ëœ ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œ --**\\n\" + \"\\n\".join(source_citation_list)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": final_answer_with_citation,\n",
        "        \"few_shot_examples\": few_shot_results,\n",
        "        \"retrieved_context\": rag_docs\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "JRYa-XeuUQjE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VRAM ê°•ì œ í•´ì œ í•¨ìˆ˜"
      ],
      "metadata": {
        "id": "jt6ynJhz69IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 6. VRAM ê°•ì œ í•´ì œ í•¨ìˆ˜\n",
        "# ==============================================================================\n",
        "\n",
        "def cleanup_vram():\n",
        "    \"\"\"ì „ì—­ ë³€ìˆ˜ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì‚­ì œí•˜ê³  GPU ìºì‹œë¥¼ ë¹„ì›Œ VRAMì„ ê°•ì œë¡œ í•´ì œí•©ë‹ˆë‹¤.\"\"\"\n",
        "    global model, tokenizer\n",
        "    print(\"\\n--- ğŸ§¹ VRAM í•´ì œ ì‹œì‘ (ê³µê²©ì  í•´ì œ) ---\")\n",
        "\n",
        "    if 'model' in globals() and model is not None:\n",
        "        print(f\"ëª¨ë¸ ê°ì²´ ({LLM_MODEL_NAME}) ì‚­ì œ...\")\n",
        "        del model\n",
        "        model = None\n",
        "    if 'tokenizer' in globals() and tokenizer is not None:\n",
        "        print(\"Tokenizer ê°ì²´ ì‚­ì œ...\")\n",
        "        del tokenizer\n",
        "        tokenizer = None\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"--- âœ… VRAM í•´ì œ ì™„ë£Œ ---\")\n"
      ],
      "metadata": {
        "id": "qFCoyOqO69Z6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë©”ì¸ ì‹¤í–‰"
      ],
      "metadata": {
        "id": "CbSib1K3xmHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 7. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡\n",
        "# ==============================================================================\n",
        "\n",
        "def extract_answer_sentence(context, answer):\n",
        "    \"\"\"ì •ë‹µ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì¶œë ¥ ê°€ë…ì„±ìš©)\"\"\"\n",
        "    if not answer or answer not in context:\n",
        "        return context[:150] + \"...\"\n",
        "\n",
        "    try:\n",
        "        start_index = context.find(answer)\n",
        "        if start_index == -1: return context[:150] + \"...\"\n",
        "\n",
        "        sentence_start = 0\n",
        "        terminal_punctuations = ['.', '?', '!']\n",
        "\n",
        "        for punc in terminal_punctuations:\n",
        "            last_punc_index = context.rfind(punc, 0, start_index)\n",
        "            if last_punc_index > sentence_start:\n",
        "                sentence_start = last_punc_index + 1\n",
        "\n",
        "        sentence_end = -1\n",
        "        for punc in terminal_punctuations:\n",
        "            punc_index = context.find(punc, start_index + len(answer))\n",
        "            if punc_index != -1 and (sentence_end == -1 or punc_index < sentence_end):\n",
        "                sentence_end = punc_index + 1\n",
        "\n",
        "        if sentence_end != -1:\n",
        "            snippet = context[sentence_start:sentence_end].strip()\n",
        "        else:\n",
        "            snippet = context[sentence_start:].strip()\n",
        "\n",
        "        if len(snippet) > 500:\n",
        "            return snippet[:500] + \"...\"\n",
        "\n",
        "        return snippet\n",
        "\n",
        "    except Exception:\n",
        "        return context[:150] + f\"... (ì¶”ì¶œ ì˜¤ë¥˜)\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. DB ë° ëª¨ë¸ ë¡œë“œ\n",
        "    setup_database()\n",
        "    load_models()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Few-Shot RAG ì‹œìŠ¤í…œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # ì‹¤í–‰ ì§ˆë¬¸\n",
        "    query_1 = \"ì´ìˆœì‹  ì¥êµ°ë‹˜ì´ ì‚¬ë§í•œ ì „ìŸì˜ ì´ë¦„ì€?\"\n",
        "\n",
        "    # 2. RAG ì‹¤í–‰ (Few-Shot K=4, RAG Context K=1)\n",
        "    try:\n",
        "        outputs_1 = few_shot_rag_invoke(query_1, k_fewshot=4, k_rag=1)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
        "        cleanup_vram()\n",
        "        exit()\n",
        "\n",
        "    # 3. ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(\"Few-Shot RAG ìµœì¢… ê²°ê³¼\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(f\"â¡ï¸ ì§ˆë¬¸: {outputs_1['query']}\")\n",
        "    print(f\"âœ… ë‹µë³€: {outputs_1['answer']}\")\n",
        "\n",
        "    print(\"\\n--- Few-Shot ì˜ˆì‹œ (ì§ˆë¬¸ ê¸°ë°˜ ê²€ìƒ‰) ---\")\n",
        "\n",
        "    # Few-Shot ì˜ˆì‹œ ì¶œë ¥\n",
        "    for i, ex in enumerate(outputs_1['few_shot_examples']):\n",
        "        answer = ex.metadata.get('answer', '')\n",
        "        snippet = extract_answer_sentence(ex.page_content, answer)\n",
        "\n",
        "        print(f\"  {i+1}. ìœ ì‚¬ ì§ˆë¬¸: {ex.metadata['question']}\")\n",
        "        print(f\"     ë¬¸ì„œ ì •ë‹µ: {answer}\")\n",
        "        print(f\"     ì»¨í…ìŠ¤íŠ¸ (ì¶”ì¶œ): {snippet}\")\n",
        "\n",
        "    print(\"\\n--- RAG ì»¨í…ìŠ¤íŠ¸ (ë‹µë³€ ê¸°ë°˜ ê²€ìƒ‰) ---\")\n",
        "\n",
        "    # RAG ì»¨í…ìŠ¤íŠ¸ ì¶œë ¥\n",
        "    for i, doc in enumerate(outputs_1['retrieved_context']):\n",
        "        answer = doc.metadata.get('answer', '')\n",
        "        snippet = extract_answer_sentence(doc.page_content, answer)\n",
        "\n",
        "        print(f\"  {i+1}. ìœ ì‚¬ ì§ˆë¬¸: {doc.metadata.get('question', 'ì •ë³´ ì—†ìŒ')}\")\n",
        "        print(f\"     ë¬¸ì„œ ì •ë‹µ (ì°¸ê³ ): {answer}\")\n",
        "        print(f\"     ë‚´ìš© (ì¶”ì¶œ): {snippet}...\")\n",
        "\n",
        "    # 4. í…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ VRAM ì •ë¦¬\n",
        "    cleanup_vram()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "d1720338c2ec40d7a12442e927177a53",
            "a2a63bed66cc449d8c8c7908b00a000e",
            "9f03a81489354b7abd6ea16654f90423",
            "f437011168c94c55a18cc928c1b42477",
            "bc5fa5ed59cc4fd6bda4d701e1ad81e2",
            "7ea1dc49ea144e558a540c2ea9914fd9",
            "5ea96385ceba431ab623ab9f3acb86cf",
            "11753519b5404f9b890770a4ef2c9a25",
            "5417cda0e31d4b9d82b409279f206635",
            "d5ae8c28840441369b53333cf9d0b776",
            "ea44912770324649a562650c35a73833"
          ]
        },
        "id": "z7l76fKsxl28",
        "outputId": "07cc6462-f646-4133-90a3-fb0b70023ed7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ê¸°ì¡´ DB ë¡œë“œ: ./chroma_db_korquad_rag\n",
            "LLM ë¡œë“œ ì¤‘: kakaocorp/kanana-1.5-8b-instruct-2505\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1720338c2ec40d7a12442e927177a53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LLM ë¡œë“œ ë° ì„¤ì • ì™„ë£Œ.\n",
            "\n",
            "==================================================\n",
            "Few-Shot RAG ì‹œìŠ¤í…œ ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
            "==================================================\n",
            "\n",
            "RAG ì‹¤í–‰: ì§ˆë¬¸='ì´ìˆœì‹  ì¥êµ°ë‹˜ì´ ì‚¬ë§í•œ ì „ìŸì˜ ì´ë¦„ì€?' (Few-Shot K=4, RAG K=1 ë¬¸ì„œ ê²€ìƒ‰)\n",
            "\n",
            "\n",
            "==================================================\n",
            "Few-Shot RAG ìµœì¢… ê²°ê³¼\n",
            "==================================================\n",
            "â¡ï¸ ì§ˆë¬¸: ì´ìˆœì‹  ì¥êµ°ë‹˜ì´ ì‚¬ë§í•œ ì „ìŸì˜ ì´ë¦„ì€?\n",
            "âœ… ë‹µë³€: ë…¸ëŸ‰í•´ì „\n",
            "\n",
            "**-- ì°¸ì¡°ëœ ìœ„í‚¤í”¼ë””ì•„ ë¬¸ì„œ --**\n",
            "[1] ì´ìˆœì‹ : ì´ìˆœì‹ ì´ ì „ì‚¬í•œ ê³³ì€?\n",
            "[2] ì´ìˆœì‹ : ì´ìˆœì‹ ì˜ ì¥ì¸ì€ ëˆ„êµ¬ì¸ê°€?\n",
            "\n",
            "--- Few-Shot ì˜ˆì‹œ (ì§ˆë¬¸ ê¸°ë°˜ ê²€ìƒ‰) ---\n",
            "  1. ìœ ì‚¬ ì§ˆë¬¸: ì´ìˆœì‹ ì´ ì „ì‚¬í•œ ê³³ì€?\n",
            "     ë¬¸ì„œ ì •ë‹µ: ë…¸ëŸ‰í•´í˜‘\n",
            "     ì»¨í…ìŠ¤íŠ¸ (ì¶”ì¶œ): ì´ìˆœì‹ ì€ ëª…ë‚˜ë¼ ë¶€ì´ë³‘ ì§„ë¦°(é™³ç’˜)ê³¼ í•¨ê»˜ 1598ë…„ ìŒë ¥ 11ì›” 19ì¼ ìƒˆë²½ë¶€í„° ë…¸ëŸ‰í•´í˜‘ì— ëª¨ì—¬ ìˆëŠ” ì¼ë³¸êµ°ì„ ê³µê²©í•˜ì˜€ê³ , ì¼ë³¸ìœ¼ë¡œ ê±´ë„ˆê°ˆ ì¤€ë¹„ë¥¼ í•˜ê³  ìˆë˜ ì™œêµ° ì„ ë‹¨ 500ì—¬ ì²™ ê°€ìš´ë° 200ì—¬ ì²™ì„ ê²©íŒŒ, 150ì—¬ ì²™ì„ íŒŒì†ì‹œì¼°ë‹¤.\n",
            "  2. ìœ ì‚¬ ì§ˆë¬¸: ì´ìˆœì‹ ì˜ ì¥ì¸ì€ ëˆ„êµ¬ì¸ê°€?\n",
            "     ë¬¸ì„œ ì •ë‹µ: ë°©ì§„\n",
            "     ì»¨í…ìŠ¤íŠ¸ (ì¶”ì¶œ): ì¥ì¸ì€ ë³´ì„±êµ°ìˆ˜ë¥¼ ì—­ì„í•œ ë°©ì§„(æ–¹éœ‡)ì´ë‹¤.\n",
            "\n",
            "--- RAG ì»¨í…ìŠ¤íŠ¸ (ë‹µë³€ ê¸°ë°˜ ê²€ìƒ‰) ---\n",
            "\n",
            "--- ğŸ§¹ VRAM í•´ì œ ì‹œì‘ (ê³µê²©ì  í•´ì œ) ---\n",
            "ëª¨ë¸ ê°ì²´ (kakaocorp/kanana-1.5-8b-instruct-2505) ì‚­ì œ...\n",
            "Tokenizer ê°ì²´ ì‚­ì œ...\n",
            "--- âœ… VRAM í•´ì œ ì™„ë£Œ ---\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d1720338c2ec40d7a12442e927177a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2a63bed66cc449d8c8c7908b00a000e",
              "IPY_MODEL_9f03a81489354b7abd6ea16654f90423",
              "IPY_MODEL_f437011168c94c55a18cc928c1b42477"
            ],
            "layout": "IPY_MODEL_bc5fa5ed59cc4fd6bda4d701e1ad81e2"
          }
        },
        "a2a63bed66cc449d8c8c7908b00a000e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea1dc49ea144e558a540c2ea9914fd9",
            "placeholder": "​",
            "style": "IPY_MODEL_5ea96385ceba431ab623ab9f3acb86cf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9f03a81489354b7abd6ea16654f90423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11753519b5404f9b890770a4ef2c9a25",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5417cda0e31d4b9d82b409279f206635",
            "value": 4
          }
        },
        "f437011168c94c55a18cc928c1b42477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5ae8c28840441369b53333cf9d0b776",
            "placeholder": "​",
            "style": "IPY_MODEL_ea44912770324649a562650c35a73833",
            "value": " 4/4 [00:17&lt;00:00,  3.77s/it]"
          }
        },
        "bc5fa5ed59cc4fd6bda4d701e1ad81e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea1dc49ea144e558a540c2ea9914fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea96385ceba431ab623ab9f3acb86cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11753519b5404f9b890770a4ef2c9a25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5417cda0e31d4b9d82b409279f206635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5ae8c28840441369b53333cf9d0b776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea44912770324649a562650c35a73833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/dragonkue/BGE-m3-ko"
      ],
      "metadata": {
        "id": "2G3wuhljndZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/kakaocorp/kanana-1.5-8b-base"
      ],
      "metadata": {
        "id": "O0BSpS3bnfCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전체 코드 작성"
      ],
      "metadata": {
        "id": "3_TDWUa3Tmbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 환경 설정 및 라이브러리 설치"
      ],
      "metadata": {
        "id": "kOe0vKu9Tq5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 1. 환경 설정 및 라이브러리 설치\n",
        "# ==============================================================================\n",
        "\n",
        "# 필요한 라이브러리 설치\n",
        "!pip install -q datasets sentence-transformers chromadb transformers torch accelerate bitsandbytes pandas langchain_community langchain-core\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from typing import Dict, Any, List\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "L3TTt4dLTmB_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 데이터 로드 및 전처리"
      ],
      "metadata": {
        "id": "rSqtwPlBTyMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. 전역 설정 및 모델 정보\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Global Configurations ---\n",
        "LLM_MODEL_NAME = \"kakaocorp/kanana-1.5-8b-instruct-2505\"\n",
        "EMBEDDING_MODEL_NAME = \"dragonkue/bge-m3-ko\"\n",
        "CHROMA_DB_PATH = \"./chroma_db_korquad_rag\"\n",
        "\n",
        "# --- LLM Quantization Configuration (4-bit) ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# --- Global Variables for Loaded Resources ---\n",
        "tokenizer = None\n",
        "model = None\n",
        "unified_db = None\n",
        "\n",
        "# --- Few-Shot/RAG Prompt Templates ---\n",
        "\n",
        "# Few-Shot 예시 템플릿\n",
        "example_template = \"\"\"\n",
        "[예시 질문]: {question}\n",
        "[예시 출처]: {context}\n",
        "[예시 답변]: {answer}\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\", \"context\"], template=example_template)\n",
        "\n",
        "# 최종 RAG 프롬프트 템플릿\n",
        "final_rag_template_fewshot = \"\"\"\n",
        "## 임무:\n",
        "\n",
        "제공된 '최종 출처' 정보만을 사용하여 '최종 질문'에 가장 정확하게 답변하십시오. 출처에 답이 없으면 '주어진 정보로는 답을 찾을 수 없습니다.'라고 답하세요. 답변은 반드시 출처에 명시된 용어로 작성하십시오.\n",
        "\n",
        "## 학습 예시:\n",
        "{few_shot_examples}\n",
        "\n",
        "## 최종 출처:\n",
        "{context}\n",
        "\n",
        "## 최종 질문:\n",
        "{question}\n",
        "\n",
        "[최종 답변]:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WNeWv_q_TvxP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 임베딩 모델 및 LLM 로드"
      ],
      "metadata": {
        "id": "ulM88QC9T6e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. 데이터 처리 및 벡터 DB 구축 함수\n",
        "# ==============================================================================\n",
        "\n",
        "def setup_database():\n",
        "    \"\"\"KorQuAD 데이터를 로드하고 ChromaDB를 구축 또는 로드합니다.\"\"\"\n",
        "    global unified_db\n",
        "\n",
        "    if os.path.exists(CHROMA_DB_PATH):\n",
        "        print(f\"✅ 기존 DB 로드: {CHROMA_DB_PATH}\")\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "        unified_db = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)\n",
        "        return\n",
        "\n",
        "    print(\"DB 구축 시작: KorQuAD 데이터 로드 및 임베딩...\")\n",
        "\n",
        "    data = load_dataset('squad_kor_v1', split='train')\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    full_context_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "            full_context_data.append({\n",
        "                \"question\": row['question'],\n",
        "                \"answer\": row['answers']['text'][0],\n",
        "                \"context\": row['context'],\n",
        "                \"title\": row['title']\n",
        "            })\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    processed_df = pd.DataFrame(full_context_data)\n",
        "    rag_df = processed_df.drop_duplicates(subset=['question', 'context'])\n",
        "\n",
        "    rag_docs = []\n",
        "    for index, row in rag_df.iterrows():\n",
        "        rag_docs.append(\n",
        "            Document(\n",
        "                page_content=row['context'],\n",
        "                metadata={\n",
        "                    \"question\": row['question'],\n",
        "                    \"answer\": row['answer'],\n",
        "                    \"title\": row['title']\n",
        "                }\n",
        "            )\n",
        "        )\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "    unified_db = Chroma.from_documents(\n",
        "        rag_docs,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=CHROMA_DB_PATH\n",
        "    )\n",
        "    unified_db.persist()\n",
        "    print(\"✅ 원문 Context 기반 통합 DB 구축 완료.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "qN86XRqcT7Jm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 벡터 데이터베이스(ChromaDB) 구축"
      ],
      "metadata": {
        "id": "ggDQD14jUFMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. LLM 로드 및 Tokenizer 설정\n",
        "# ==============================================================================\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"LLM 및 Tokenizer를 로드하고 설정합니다. (중복 로드 방지)\"\"\"\n",
        "    global tokenizer, model\n",
        "\n",
        "    if model is not None:\n",
        "        print(\"✅ 모델이 이미 로드되어 있습니다. 로딩을 건너뜜.\")\n",
        "        return\n",
        "\n",
        "    print(f\"LLM 로드 중: {LLM_MODEL_NAME}\")\n",
        "\n",
        "    try:\n",
        "        # 1. Tokenizer 설정 (제공된 모델 설정값 반영)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "        tokenizer.bos_token_id = 128000\n",
        "        tokenizer.eos_token_id = 128009\n",
        "        tokenizer.pad_token_id = 128001\n",
        "\n",
        "        # 2. LLM 로드 (4-bit 양자화)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            LLM_MODEL_NAME,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        model.eval()\n",
        "        print(\"✅ LLM 로드 및 설정 완료.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ LLM 로드 실패: {e}\")\n",
        "        raise RuntimeError(\"LLM 로드 실패. Colab 환경 (GPU) 및 라이브러리 설치를 확인하세요.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vQxx-ucpUK_8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. RAG 모델 설정 및 실행"
      ],
      "metadata": {
        "id": "h4IIa3-2UP0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 5. RAG 추론 함수 (Few-Shot RAG)\n",
        "# ==============================================================================\n",
        "\n",
        "def few_shot_rag_invoke(query: str, k_fewshot: int = 1, k_rag: int = 4) -> Dict[str, Any]:\n",
        "    \"\"\"Few-Shot 예시 컨텍스트를 최종 출처에 포함하여 RAG 추론을 실행합니다.\"\"\"\n",
        "\n",
        "    if unified_db is None or model is None:\n",
        "        raise RuntimeError(\"RAG 시스템이 초기화되지 않았습니다.\")\n",
        "\n",
        "    print(f\"\\nRAG 실행: 질문='{query}' (Few-Shot K={k_fewshot}, RAG K={k_rag} 문서 검색)\")\n",
        "\n",
        "    # 1. Few-Shot & RAG Context 통합 검색 및 중복 제거\n",
        "    search_results = unified_db.similarity_search(query, k=k_fewshot + k_rag + 5)\n",
        "\n",
        "    unique_results = []\n",
        "    seen_content = set()\n",
        "    for doc in search_results:\n",
        "        if doc.page_content not in seen_content:\n",
        "            unique_results.append(doc)\n",
        "            seen_content.add(doc.page_content)\n",
        "\n",
        "    # 검색 결과 분할: Few-Shot 예시와 RAG Context\n",
        "    few_shot_results = unique_results[:k_fewshot]\n",
        "    rag_docs = unique_results[k_fewshot:k_fewshot + k_rag]\n",
        "\n",
        "    # 2. Few-Shot 예시 텍스트 구성 (모델 학습용)\n",
        "    few_shot_examples_text = \"\".join([\n",
        "        example_prompt.format(\n",
        "            question=doc.metadata['question'],\n",
        "            answer=doc.metadata['answer'],\n",
        "            context=doc.page_content\n",
        "        ) for doc in few_shot_results\n",
        "    ])\n",
        "\n",
        "    # 3. RAG Context 구성 (최종 출처: Few-Shot 컨텍스트 + RAG 컨텍스트)\n",
        "\n",
        "    # Few-Shot 예시 문서의 컨텍스트를 가져옴\n",
        "    few_shot_contexts = [doc.page_content for doc in few_shot_results]\n",
        "    # RAG 문서의 컨텍스트를 가져옴\n",
        "    rag_contexts = [doc.page_content for doc in rag_docs]\n",
        "\n",
        "    # 두 리스트를 합쳐 최종 컨텍스트 리스트를 만들고 병합\n",
        "    all_contexts = few_shot_contexts + rag_contexts\n",
        "    rag_context = \"\\n---\\n\".join(all_contexts)\n",
        "\n",
        "    # 4. 최종 프롬프트 Content 구성 (Few-Shot 템플릿 사용)\n",
        "    final_prompt_content = final_rag_template_fewshot.format(\n",
        "        few_shot_examples=few_shot_examples_text,\n",
        "        context=rag_context,\n",
        "        question=query\n",
        "    )\n",
        "\n",
        "    # 5. LLM 입력 준비 및 추론\n",
        "    messages = [{\"role\": \"user\", \"content\": final_prompt_content}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128, do_sample=False, eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # 6. 답변 디코딩 및 클리닝\n",
        "    decoded_answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True).strip()\n",
        "    clean_answer = decoded_answer\n",
        "    stop_tokens = [\"assistant\", \"[최종 답변]:\", \"user:\"]\n",
        "    for token in stop_tokens:\n",
        "        if token in clean_answer:\n",
        "            clean_answer = clean_answer.split(token)[0].strip()\n",
        "\n",
        "    # 7. 최종 결과 포맷팅 및 출처 표기 추가\n",
        "    source_citation_list = []\n",
        "\n",
        "    # Few-Shot 예시 문서 포맷팅\n",
        "    for i, doc in enumerate(few_shot_results):\n",
        "        # 문서 제목: 유사 질문 (Few-Shot 예시) 형식\n",
        "        citation_text = f\"[{i+1}] {doc.metadata.get('title', '정보 없음')}: {doc.metadata.get('question', '질문 없음')}\"\n",
        "        source_citation_list.append(citation_text)\n",
        "\n",
        "    # RAG Context 문서 포맷팅\n",
        "    rag_doc_start_index = len(few_shot_results)\n",
        "    for i, doc in enumerate(rag_docs):\n",
        "        # 문서 제목: 질문 형식\n",
        "        citation_text = f\"[{i + rag_doc_start_index + 1}] {doc.metadata.get('title', '정보 없음')}: {doc.metadata.get('question', '질문 없음')}\"\n",
        "        source_citation_list.append(citation_text)\n",
        "\n",
        "    # LLM 답변과 출처 목록을 결합\n",
        "    final_answer_with_citation = clean_answer\n",
        "    if source_citation_list:\n",
        "        final_answer_with_citation += \"\\n\\n**-- 참조된 위키피디아 문서 --**\\n\" + \"\\n\".join(source_citation_list)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"answer\": final_answer_with_citation,\n",
        "        \"few_shot_examples\": few_shot_results,\n",
        "        \"retrieved_context\": rag_docs\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "JRYa-XeuUQjE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VRAM 강제 해제 함수"
      ],
      "metadata": {
        "id": "jt6ynJhz69IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 6. VRAM 강제 해제 함수\n",
        "# ==============================================================================\n",
        "\n",
        "def cleanup_vram():\n",
        "    \"\"\"전역 변수 모델과 토크나이저를 삭제하고 GPU 캐시를 비워 VRAM을 강제로 해제합니다.\"\"\"\n",
        "    global model, tokenizer\n",
        "    print(\"\\n--- 🧹 VRAM 해제 시작 (공격적 해제) ---\")\n",
        "\n",
        "    if 'model' in globals() and model is not None:\n",
        "        print(f\"모델 객체 ({LLM_MODEL_NAME}) 삭제...\")\n",
        "        del model\n",
        "        model = None\n",
        "    if 'tokenizer' in globals() and tokenizer is not None:\n",
        "        print(\"Tokenizer 객체 삭제...\")\n",
        "        del tokenizer\n",
        "        tokenizer = None\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"--- ✅ VRAM 해제 완료 ---\")\n"
      ],
      "metadata": {
        "id": "qFCoyOqO69Z6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 메인 실행"
      ],
      "metadata": {
        "id": "CbSib1K3xmHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 7. 메인 실행 블록\n",
        "# ==============================================================================\n",
        "\n",
        "def extract_answer_sentence(context, answer):\n",
        "    \"\"\"정답 텍스트를 포함하는 문장을 추출합니다. (출력 가독성용)\"\"\"\n",
        "    if not answer or answer not in context:\n",
        "        return context[:150] + \"...\"\n",
        "\n",
        "    try:\n",
        "        start_index = context.find(answer)\n",
        "        if start_index == -1: return context[:150] + \"...\"\n",
        "\n",
        "        sentence_start = 0\n",
        "        terminal_punctuations = ['.', '?', '!']\n",
        "\n",
        "        for punc in terminal_punctuations:\n",
        "            last_punc_index = context.rfind(punc, 0, start_index)\n",
        "            if last_punc_index > sentence_start:\n",
        "                sentence_start = last_punc_index + 1\n",
        "\n",
        "        sentence_end = -1\n",
        "        for punc in terminal_punctuations:\n",
        "            punc_index = context.find(punc, start_index + len(answer))\n",
        "            if punc_index != -1 and (sentence_end == -1 or punc_index < sentence_end):\n",
        "                sentence_end = punc_index + 1\n",
        "\n",
        "        if sentence_end != -1:\n",
        "            snippet = context[sentence_start:sentence_end].strip()\n",
        "        else:\n",
        "            snippet = context[sentence_start:].strip()\n",
        "\n",
        "        if len(snippet) > 500:\n",
        "            return snippet[:500] + \"...\"\n",
        "\n",
        "        return snippet\n",
        "\n",
        "    except Exception:\n",
        "        return context[:150] + f\"... (추출 오류)\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. DB 및 모델 로드\n",
        "    setup_database()\n",
        "    load_models()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Few-Shot RAG 시스템 실행 테스트\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 실행 질문\n",
        "    query_1 = \"이순신 장군님이 사망한 전쟁의 이름은?\"\n",
        "\n",
        "    # 2. RAG 실행 (Few-Shot K=4, RAG Context K=1)\n",
        "    try:\n",
        "        outputs_1 = few_shot_rag_invoke(query_1, k_fewshot=4, k_rag=1)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"\\n❌ 실행 오류: {e}\")\n",
        "        cleanup_vram()\n",
        "        exit()\n",
        "\n",
        "    # 3. 최종 결과 출력\n",
        "    print(\"\\n\\n\" + \"=\"*50)\n",
        "    print(\"Few-Shot RAG 최종 결과\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(f\"➡️ 질문: {outputs_1['query']}\")\n",
        "    print(f\"✅ 답변: {outputs_1['answer']}\")\n",
        "\n",
        "    print(\"\\n--- Few-Shot 예시 (질문 기반 검색) ---\")\n",
        "\n",
        "    # Few-Shot 예시 출력\n",
        "    for i, ex in enumerate(outputs_1['few_shot_examples']):\n",
        "        answer = ex.metadata.get('answer', '')\n",
        "        snippet = extract_answer_sentence(ex.page_content, answer)\n",
        "\n",
        "        print(f\"  {i+1}. 유사 질문: {ex.metadata['question']}\")\n",
        "        print(f\"     문서 정답: {answer}\")\n",
        "        print(f\"     컨텍스트 (추출): {snippet}\")\n",
        "\n",
        "    print(\"\\n--- RAG 컨텍스트 (답변 기반 검색) ---\")\n",
        "\n",
        "    # RAG 컨텍스트 출력\n",
        "    for i, doc in enumerate(outputs_1['retrieved_context']):\n",
        "        answer = doc.metadata.get('answer', '')\n",
        "        snippet = extract_answer_sentence(doc.page_content, answer)\n",
        "\n",
        "        print(f\"  {i+1}. 유사 질문: {doc.metadata.get('question', '정보 없음')}\")\n",
        "        print(f\"     문서 정답 (참고): {answer}\")\n",
        "        print(f\"     내용 (추출): {snippet}...\")\n",
        "\n",
        "    # 4. 테스트 완료 후 VRAM 정리\n",
        "    cleanup_vram()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "d1720338c2ec40d7a12442e927177a53",
            "a2a63bed66cc449d8c8c7908b00a000e",
            "9f03a81489354b7abd6ea16654f90423",
            "f437011168c94c55a18cc928c1b42477",
            "bc5fa5ed59cc4fd6bda4d701e1ad81e2",
            "7ea1dc49ea144e558a540c2ea9914fd9",
            "5ea96385ceba431ab623ab9f3acb86cf",
            "11753519b5404f9b890770a4ef2c9a25",
            "5417cda0e31d4b9d82b409279f206635",
            "d5ae8c28840441369b53333cf9d0b776",
            "ea44912770324649a562650c35a73833"
          ]
        },
        "id": "z7l76fKsxl28",
        "outputId": "07cc6462-f646-4133-90a3-fb0b70023ed7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 기존 DB 로드: ./chroma_db_korquad_rag\n",
            "LLM 로드 중: kakaocorp/kanana-1.5-8b-instruct-2505\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1720338c2ec40d7a12442e927177a53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LLM 로드 및 설정 완료.\n",
            "\n",
            "==================================================\n",
            "Few-Shot RAG 시스템 실행 테스트\n",
            "==================================================\n",
            "\n",
            "RAG 실행: 질문='이순신 장군님이 사망한 전쟁의 이름은?' (Few-Shot K=4, RAG K=1 문서 검색)\n",
            "\n",
            "\n",
            "==================================================\n",
            "Few-Shot RAG 최종 결과\n",
            "==================================================\n",
            "➡️ 질문: 이순신 장군님이 사망한 전쟁의 이름은?\n",
            "✅ 답변: 노량해전\n",
            "\n",
            "**-- 참조된 위키피디아 문서 --**\n",
            "[1] 이순신: 이순신이 전사한 곳은?\n",
            "[2] 이순신: 이순신의 장인은 누구인가?\n",
            "\n",
            "--- Few-Shot 예시 (질문 기반 검색) ---\n",
            "  1. 유사 질문: 이순신이 전사한 곳은?\n",
            "     문서 정답: 노량해협\n",
            "     컨텍스트 (추출): 이순신은 명나라 부총병 진린(陳璘)과 함께 1598년 음력 11월 19일 새벽부터 노량해협에 모여 있는 일본군을 공격하였고, 일본으로 건너갈 준비를 하고 있던 왜군 선단 500여 척 가운데 200여 척을 격파, 150여 척을 파손시켰다.\n",
            "  2. 유사 질문: 이순신의 장인은 누구인가?\n",
            "     문서 정답: 방진\n",
            "     컨텍스트 (추출): 장인은 보성군수를 역임한 방진(方震)이다.\n",
            "\n",
            "--- RAG 컨텍스트 (답변 기반 검색) ---\n",
            "\n",
            "--- 🧹 VRAM 해제 시작 (공격적 해제) ---\n",
            "모델 객체 (kakaocorp/kanana-1.5-8b-instruct-2505) 삭제...\n",
            "Tokenizer 객체 삭제...\n",
            "--- ✅ VRAM 해제 완료 ---\n"
          ]
        }
      ]
    }
  ]
}